"""
–°–µ—Ä–≤–∏—Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–Ω—ã—Ö ML –º–æ–¥–µ–ª–µ–π
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç GridSearchCV –∏ RandomizedSearchCV –¥–ª—è –ø–æ–∏—Å–∫–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, TimeSeriesSplit
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, r2_score
from flask import current_app
import joblib
import time
from typing import Dict, Any, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

try:
    import xgboost as xgb
    XGB_AVAILABLE = True
except ImportError:
    XGB_AVAILABLE = False
    xgb = None


class HyperparameterOptimizationService:
    """
    –°–µ—Ä–≤–∏—Å –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ ML –º–æ–¥–µ–ª–µ–π
    """
    
    def __init__(self):
        self.best_params = {}
        self.optimization_results = {}
        
    def get_parameter_grids(self) -> Dict[str, Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å–µ—Ç–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        """
        param_grids = {
            'rf': {
                'n_estimators': [100, 200, 300],
                'max_depth': [10, 15, 20, None],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4],
                'max_features': ['sqrt', 'log2', None],
                'bootstrap': [True, False]
            },
            'gb': {
                'n_estimators': [100, 200, 300],
                'learning_rate': [0.01, 0.1, 0.2],
                'max_depth': [3, 5, 7],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4],
                'subsample': [0.8, 0.9, 1.0]
            }
        }
        
        if XGB_AVAILABLE:
            param_grids['xgb'] = {
                'n_estimators': [100, 200, 300],
                'learning_rate': [0.01, 0.1, 0.2],
                'max_depth': [3, 5, 7],
                'min_child_weight': [1, 3, 5],
                'subsample': [0.8, 0.9, 1.0],
                'colsample_bytree': [0.8, 0.9, 1.0],
                'reg_alpha': [0, 0.1, 0.5],
                'reg_lambda': [1, 1.5, 2]
            }
            
        return param_grids
    
    def get_reduced_parameter_grids(self) -> Dict[str, Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —É–º–µ–Ω—å—à–µ–Ω–Ω—ã–µ —Å–µ—Ç–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        """
        param_grids = {
            'rf': {
                'n_estimators': [100, 200],
                'max_depth': [10, 15, None],
                'min_samples_split': [2, 5],
                'min_samples_leaf': [1, 2],
                'max_features': ['sqrt', None]
            },
            'gb': {
                'n_estimators': [100, 200],
                'learning_rate': [0.1, 0.2],
                'max_depth': [3, 5],
                'min_samples_split': [2, 5],
                'subsample': [0.8, 1.0]
            }
        }
        
        if XGB_AVAILABLE:
            param_grids['xgb'] = {
                'n_estimators': [100, 200],
                'learning_rate': [0.1, 0.2],
                'max_depth': [3, 5],
                'min_child_weight': [1, 3],
                'subsample': [0.8, 1.0],
                'colsample_bytree': [0.8, 1.0]
            }
            
        return param_grids
    
    def optimize_model_hyperparameters(
        self, 
        model_name: str, 
        X_train: pd.DataFrame, 
        y_train: pd.Series,
        use_reduced_grid: bool = True,
        cv_folds: int = 3,
        n_jobs: int = -1,
        scoring: str = 'neg_mean_absolute_error'
    ) -> Tuple[Any, Dict[str, Any], Dict[str, float]]:
        """
        –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏
        """
        current_app.logger.info(f"üîß –ù–∞—á–∏–Ω–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è {model_name}")
        start_time = time.time()
        
        # –ü–æ–ª—É—á–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å
        base_model = self._get_base_model(model_name)
        if base_model is None:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–∞—è –º–æ–¥–µ–ª—å: {model_name}")
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–µ—Ç–∫—É –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        param_grids = self.get_reduced_parameter_grids() if use_reduced_grid else self.get_parameter_grids()
        param_grid = param_grids.get(model_name, {})
        
        if not param_grid:
            current_app.logger.warning(f"–ù–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ {model_name}")
            return base_model, {}, {}
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—é –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
        tscv = TimeSeriesSplit(n_splits=cv_folds)
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º GridSearch
        grid_search = GridSearchCV(
            estimator=base_model,
            param_grid=param_grid,
            cv=tscv,
            scoring=scoring,
            n_jobs=n_jobs,
            verbose=0,
            error_score='raise'
        )
        
        try:
            # –û—á–∏—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            X_clean = X_train.fillna(0).replace([np.inf, -np.inf], 0)
            y_clean = y_train.fillna(y_train.mean())
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
            grid_search.fit(X_clean, y_clean)
            
            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            best_model = grid_search.best_estimator_
            best_params = grid_search.best_params_
            best_score = -grid_search.best_score_  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ –∏–∑ negative MAE
            
            # –í—ã—á–∏—Å–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            y_pred = best_model.predict(X_clean)
            r2 = r2_score(y_clean, y_pred)
            mae = mean_absolute_error(y_clean, y_pred)
            
            metrics = {
                'best_cv_mae': best_score,
                'train_mae': mae,
                'train_r2': r2,
                'optimization_time': time.time() - start_time
            }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            self.best_params[model_name] = best_params
            self.optimization_results[model_name] = {
                'best_params': best_params,
                'metrics': metrics,
                'cv_results': grid_search.cv_results_
            }
            
            current_app.logger.info(
                f"‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è {model_name} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {metrics['optimization_time']:.2f}—Å. "
                f"–õ—É—á—à–∏–π CV MAE: {best_score:.2f}, R¬≤: {r2:.3f}"
            )
            
            return best_model, best_params, metrics
            
        except Exception as e:
            current_app.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ {model_name}: {e}")
            return base_model, {}, {'error': str(e)}
    
    def optimize_ensemble_hyperparameters(
        self,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        models_to_optimize: list = None,
        use_reduced_grid: bool = True,
        cv_folds: int = 3
    ) -> Dict[str, Any]:
        """
        –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –≤ –∞–Ω—Å–∞–º–±–ª–µ
        """
        if models_to_optimize is None:
            models_to_optimize = ['rf', 'gb']
            if XGB_AVAILABLE:
                models_to_optimize.append('xgb')
        
        current_app.logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π: {models_to_optimize}")
        
        optimized_models = {}
        optimization_summary = {}
        
        for model_name in models_to_optimize:
            try:
                model, params, metrics = self.optimize_model_hyperparameters(
                    model_name=model_name,
                    X_train=X_train,
                    y_train=y_train,
                    use_reduced_grid=use_reduced_grid,
                    cv_folds=cv_folds
                )
                
                optimized_models[model_name] = model
                optimization_summary[model_name] = {
                    'best_params': params,
                    'metrics': metrics
                }
                
            except Exception as e:
                current_app.logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å {model_name}: {e}")
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
                base_model = self._get_base_model(model_name)
                if base_model:
                    optimized_models[model_name] = base_model
                    optimization_summary[model_name] = {
                        'best_params': {},
                        'metrics': {'error': str(e)}
                    }
        
        return {
            'optimized_models': optimized_models,
            'optimization_summary': optimization_summary,
            'total_models_optimized': len(optimized_models)
        }
    
    def _get_base_model(self, model_name: str) -> Optional[Any]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –ø–æ –∏–º–µ–Ω–∏
        """
        if model_name == 'rf':
            return RandomForestRegressor(random_state=42, n_jobs=-1)
        elif model_name == 'gb':
            return GradientBoostingRegressor(random_state=42)
        elif model_name == 'xgb' and XGB_AVAILABLE:
            return xgb.XGBRegressor(
                random_state=42,
                n_jobs=-1,
                verbosity=0,
                objective='reg:squarederror'
            )
        else:
            return None
    
    def save_optimization_results(self, filepath: str):
        """
        –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        """
        try:
            results = {
                'best_params': self.best_params,
                'optimization_results': self.optimization_results
            }
            joblib.dump(results, filepath)
            current_app.logger.info(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filepath}")
        except Exception as e:
            current_app.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {e}")
    
    def load_optimization_results(self, filepath: str) -> bool:
        """
        –ó–∞–≥—Ä—É–∑–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        """
        try:
            results = joblib.load(filepath)
            self.best_params = results.get('best_params', {})
            self.optimization_results = results.get('optimization_results', {})
            current_app.logger.info(f"üìÇ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ {filepath}")
            return True
        except Exception as e:
            current_app.logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {e}")
            return False
    
    def get_optimization_report(self) -> str:
        """
        –ü–æ–ª—É—á–∏—Ç—å –æ—Ç—á–µ—Ç –æ–± –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        """
        if not self.optimization_results:
            return "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –µ—â–µ –Ω–µ –≤—ã–ø–æ–ª–Ω—è–ª–∞—Å—å"
        
        report = "üîß –û–¢–ß–ï–¢ –û–ë –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–û–í\n"
        report += "=" * 60 + "\n\n"
        
        for model_name, results in self.optimization_results.items():
            report += f"üìä –ú–æ–¥–µ–ª—å: {model_name.upper()}\n"
            report += "-" * 30 + "\n"
            
            best_params = results.get('best_params', {})
            metrics = results.get('metrics', {})
            
            if best_params:
                report += "–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:\n"
                for param, value in best_params.items():
                    report += f"  {param}: {value}\n"
                report += "\n"
            
            if metrics and 'error' not in metrics:
                report += "–ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:\n"
                report += f"  CV MAE: {metrics.get('best_cv_mae', 'N/A'):.2f}\n"
                report += f"  Train MAE: {metrics.get('train_mae', 'N/A'):.2f}\n"
                report += f"  Train R¬≤: {metrics.get('train_r2', 'N/A'):.3f}\n"
                report += f"  –í—Ä–µ–º—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {metrics.get('optimization_time', 'N/A'):.2f}—Å\n"
            elif 'error' in metrics:
                report += f"‚ùå –û—à–∏–±–∫–∞: {metrics['error']}\n"
            
            report += "\n"
        
        return report
