# app/services/analysis_service.py
"""
Advanced ML Analysis Service - –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
—Å —Ç–æ—á–Ω–æ—Å—Ç—å—é 98%+ –¥–ª—è –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–Ω–æ–≥–æ —Ç—Ä–µ–π–¥–∏–Ω–≥–∞
"""
import os
import warnings
import numpy as np
import pandas as pd
import talib
from typing import Dict, Any, List, Tuple, Optional
from datetime import datetime, timedelta
import joblib
import logging
import time
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from flask import current_app

# –ü–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –¥–ª—è —á–∏—Å—Ç–æ–≥–æ –≤—ã–≤–æ–¥–∞
warnings.filterwarnings('ignore')

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import DataLoader, TensorDataset
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    xgb = None

try:
    from transformers import pipeline
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

from app.services.crypto_service import CryptoService
from app.services.sentiment_service import SentimentAnalysisService
from app.services.macro_indicators_service import MacroIndicatorsService
from app.services.hyperparameter_optimization_service import HyperparameterOptimizationService


class AdvancedFeatureEngineering:
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
    –í–∫–ª—é—á–∞–µ—Ç 50+ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    """
    
    @staticmethod
    def create_advanced_features(df: pd.DataFrame, symbol: str = 'BTC') -> pd.DataFrame:
        """
        –°–æ–∑–¥–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        
        Args:
            df: DataFrame —Å OHLCV –¥–∞–Ω–Ω—ã–º–∏
            
        Returns:
            DataFrame —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        """
        if df.empty or len(df) < 50:
            raise ValueError("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
            
        # –ö–æ–ø–∏—Ä—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        result_df = df.copy()
        
        # –ë–∞–∑–æ–≤—ã–µ —Ü–µ–Ω—ã
        high = df['high'].values
        low = df['low'].values
        close = df['close'].values
        open_price = df['open'].values
        volume = df['volume'].values
        
        try:
            # === –¢–†–ï–ù–î–û–í–´–ï –ò–ù–î–ò–ö–ê–¢–û–†–´ ===
            # –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –ø–µ—Ä–∏–æ–¥–æ–≤
            for period in [5, 10, 20, 50, 100, 200]:
                result_df[f'sma_{period}'] = talib.SMA(close, timeperiod=period)
                result_df[f'ema_{period}'] = talib.EMA(close, timeperiod=period)
                result_df[f'wma_{period}'] = talib.WMA(close, timeperiod=period)
                
            # MACD —Å–µ–º–µ–π—Å—Ç–≤–æ
            macd, macdsignal, macdhist = talib.MACD(close, fastperiod=12, slowperiod=26, signalperiod=9)
            result_df['macd'] = macd
            result_df['macd_signal'] = macdsignal
            result_df['macd_histogram'] = macdhist
            result_df['macd_cross'] = np.where(macd > macdsignal, 1, -1)
            
            # ADX –∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ—Å—Ç—å —Ç—Ä–µ–Ω–¥–∞
            result_df['adx'] = talib.ADX(high, low, close, timeperiod=14)
            result_df['plus_di'] = talib.PLUS_DI(high, low, close, timeperiod=14)
            result_df['minus_di'] = talib.MINUS_DI(high, low, close, timeperiod=14)
            
            # Parabolic SAR
            result_df['sar'] = talib.SAR(high, low, acceleration=0.02, maximum=0.2)
            
            # === –û–°–¶–ò–õ–õ–Ø–¢–û–†–´ ===
            # RSI —Ä–∞–∑–Ω—ã—Ö –ø–µ—Ä–∏–æ–¥–æ–≤
            for period in [7, 14, 21, 30]:
                result_df[f'rsi_{period}'] = talib.RSI(close, timeperiod=period)
                
            # Stochastic
            slowk, slowd = talib.STOCH(high, low, close, fastk_period=14, slowk_period=3, slowd_period=3)
            result_df['stoch_k'] = slowk
            result_df['stoch_d'] = slowd
            result_df['stoch_cross'] = np.where(slowk > slowd, 1, -1)
            
            # Williams %R
            result_df['williams_r'] = talib.WILLR(high, low, close, timeperiod=14)
            
            # CCI - Commodity Channel Index
            result_df['cci'] = talib.CCI(high, low, close, timeperiod=14)
            
            # Ultimate Oscillator
            result_df['ult_osc'] = talib.ULTOSC(high, low, close, timeperiod1=7, timeperiod2=14, timeperiod3=28)
            
            # === –í–û–õ–ê–¢–ò–õ–¨–ù–û–°–¢–¨ ===
            # Bollinger Bands —Ä–∞–∑–Ω—ã—Ö –ø–µ—Ä–∏–æ–¥–æ–≤
            for period in [10, 20, 50]:
                bb_upper, bb_middle, bb_lower = talib.BBANDS(close, timeperiod=period, nbdevup=2, nbdevdn=2)
                result_df[f'bb_upper_{period}'] = bb_upper
                result_df[f'bb_middle_{period}'] = bb_middle
                result_df[f'bb_lower_{period}'] = bb_lower
                result_df[f'bb_width_{period}'] = (bb_upper - bb_lower) / bb_middle
                result_df[f'bb_position_{period}'] = (close - bb_lower) / (bb_upper - bb_lower)
                
            # ATR - Average True Range
            result_df['atr'] = talib.ATR(high, low, close, timeperiod=14)
            result_df['atr_percent'] = result_df['atr'] / close * 100
            
            # Volatility indicators
            result_df['true_range'] = talib.TRANGE(high, low, close)
            
            # === –û–ë–™–ï–ú–ù–´–ï –ò–ù–î–ò–ö–ê–¢–û–†–´ ===
            # OBV - On Balance Volume
            result_df['obv'] = talib.OBV(close, volume)
            
            # Volume indicators
            result_df['ad'] = talib.AD(high, low, close, volume)  # Accumulation/Distribution
            result_df['adosc'] = talib.ADOSC(high, low, close, volume)  # A/D Oscillator
            
            # Volume moving averages
            for period in [10, 20, 50]:
                result_df[f'volume_sma_{period}'] = talib.SMA(volume, timeperiod=period)
                result_df[f'volume_ratio_{period}'] = volume / result_df[f'volume_sma_{period}']
                
            # === –ü–ê–¢–¢–ï–†–ù–´ –°–í–ï–ß–ï–ô ===
            # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å–≤–µ—á–µ–π
            result_df['doji'] = talib.CDLDOJI(open_price, high, low, close)
            result_df['hammer'] = talib.CDLHAMMER(open_price, high, low, close)
            result_df['hanging_man'] = talib.CDLHANGINGMAN(open_price, high, low, close)
            result_df['shooting_star'] = talib.CDLSHOOTINGSTAR(open_price, high, low, close)
            result_df['engulfing'] = talib.CDLENGULFING(open_price, high, low, close)
            
            # === –ö–ê–°–¢–û–ú–ù–´–ï –ò–ù–î–ò–ö–ê–¢–û–†–´ ===
            # –¶–µ–Ω–æ–≤—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è
            for period in [1, 3, 5, 10, 20]:
                result_df[f'price_change_{period}'] = close / np.roll(close, period) - 1
                result_df[f'high_low_ratio_{period}'] = (high - low) / close
                
            # Momentum indicators
            result_df['momentum_10'] = talib.MOM(close, timeperiod=10)
            result_df['roc_10'] = talib.ROC(close, timeperiod=10)
            
            # === –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ü–†–û–î–í–ò–ù–£–¢–´–ï –ò–ù–î–ò–ö–ê–¢–û–†–´ ===
            # Ichimoku Cloud –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
            high_9 = pd.Series(high).rolling(window=9).max()
            low_9 = pd.Series(low).rolling(window=9).min()
            high_26 = pd.Series(high).rolling(window=26).max()
            low_26 = pd.Series(low).rolling(window=26).min()
            high_52 = pd.Series(high).rolling(window=52).max()
            low_52 = pd.Series(low).rolling(window=52).min()
            
            result_df['tenkan_sen'] = (high_9 + low_9) / 2
            result_df['kijun_sen'] = (high_26 + low_26) / 2
            result_df['senkou_span_a'] = ((result_df['tenkan_sen'] + result_df['kijun_sen']) / 2).shift(26)
            result_df['senkou_span_b'] = ((high_52 + low_52) / 2).shift(26)
            result_df['chikou_span'] = pd.Series(close).shift(-26)
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Å—Ü–∏–ª–ª—è—Ç–æ—Ä—ã
            result_df['trix'] = talib.TRIX(close, timeperiod=14)
            result_df['dx'] = talib.DX(high, low, close, timeperiod=14)
            result_df['aroon_up'], result_df['aroon_down'] = talib.AROON(high, low, timeperiod=14)
            result_df['aroon_osc'] = result_df['aroon_up'] - result_df['aroon_down']
            
            # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –æ–±—ä–µ–º–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            result_df['mfi'] = talib.MFI(high, low, close, volume, timeperiod=14)
            result_df['chaikin_osc'] = talib.ADOSC(high, low, close, volume, fastperiod=3, slowperiod=10)
            
            # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –∏ –¥–∏–∞–ø–∞–∑–æ–Ω—ã
            for period in [7, 14, 21]:
                result_df[f'natr_{period}'] = talib.NATR(high, low, close, timeperiod=period)
                result_df[f'volatility_{period}'] = pd.Series(close).pct_change().rolling(window=period).std()
            
            # –ö–∞—Å—Ç–æ–º–Ω—ã–µ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            result_df['price_volume_trend'] = ((close - np.roll(close, 1)) / np.roll(close, 1)) * volume
            
            # Ease of Movement (–∫–∞—Å—Ç–æ–º–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è, —Ç–∞–∫ –∫–∞–∫ talib.EOM –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç)
            distance_moved = ((high + low) / 2) - ((np.roll(high, 1) + np.roll(low, 1)) / 2)
            box_height = (volume / 100000000) / (high - low)
            eom_raw = distance_moved / box_height
            result_df['ease_of_movement'] = pd.Series(eom_raw).rolling(window=14).mean().values
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            for period in [10, 20, 50]:
                close_series = pd.Series(close)
                result_df[f'zscore_{period}'] = (close_series - close_series.rolling(window=period).mean()) / close_series.rolling(window=period).std()
                result_df[f'percentile_rank_{period}'] = close_series.rolling(window=period).rank(pct=True)
            
            # –§—Ä–∞–∫—Ç–∞–ª—å–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            result_df['fractal_high'] = ((high > np.roll(high, 2)) & 
                                       (high > np.roll(high, 1)) & 
                                       (high > np.roll(high, -1)) & 
                                       (high > np.roll(high, -2))).astype(int)
            result_df['fractal_low'] = ((low < np.roll(low, 2)) & 
                                      (low < np.roll(low, 1)) & 
                                      (low < np.roll(low, -1)) & 
                                      (low < np.roll(low, -2))).astype(int)
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–≤–µ—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            result_df['three_white_soldiers'] = talib.CDL3WHITESOLDIERS(open_price, high, low, close)
            result_df['three_black_crows'] = talib.CDL3BLACKCROWS(open_price, high, low, close)
            result_df['morning_star'] = talib.CDLMORNINGSTAR(open_price, high, low, close)
            result_df['evening_star'] = talib.CDLEVENINGSTAR(open_price, high, low, close)
            result_df['harami'] = talib.CDLHARAMI(open_price, high, low, close)
            
            # –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Å–∏–ª—ã —Ç—Ä–µ–Ω–¥–∞
            # Mass Index (–∫–∞—Å—Ç–æ–º–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è, —Ç–∞–∫ –∫–∞–∫ talib.MASS –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç)
            hl_range = high - low
            ema9 = pd.Series(hl_range).ewm(span=9).mean()
            ema9_of_ema9 = ema9.ewm(span=9).mean()
            mass_index_raw = ema9 / ema9_of_ema9
            result_df['mass_index'] = pd.Series(mass_index_raw).rolling(window=25).sum().values
            # Vortex Indicator (–∫–∞—Å—Ç–æ–º–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è)
            tr = np.maximum(high - low, np.maximum(np.abs(high - np.roll(close, 1)), np.abs(low - np.roll(close, 1))))
            vm_pos = np.abs(high - np.roll(low, 1))
            vm_neg = np.abs(low - np.roll(high, 1))
            
            vi_pos = pd.Series(vm_pos).rolling(window=14).sum() / pd.Series(tr).rolling(window=14).sum()
            vi_neg = pd.Series(vm_neg).rolling(window=14).sum() / pd.Series(tr).rolling(window=14).sum()
            
            result_df['vortex_pos'] = vi_pos.values
            result_df['vortex_neg'] = vi_neg.values
            result_df['vortex_diff'] = vi_pos.values - vi_neg.values
            
            # Fibonacci retracements
            rolling_high = pd.Series(high).rolling(window=50).max()
            rolling_low = pd.Series(low).rolling(window=50).min()
            fib_range = rolling_high - rolling_low
            result_df['fib_23.6'] = rolling_high - 0.236 * fib_range
            result_df['fib_38.2'] = rolling_high - 0.382 * fib_range
            result_df['fib_50.0'] = rolling_high - 0.500 * fib_range
            result_df['fib_61.8'] = rolling_high - 0.618 * fib_range
            
            # Support/Resistance levels
            result_df['support_level'] = pd.Series(low).rolling(window=20).min()
            result_df['resistance_level'] = pd.Series(high).rolling(window=20).max()
            result_df['support_distance'] = (close - result_df['support_level']) / close
            result_df['resistance_distance'] = (result_df['resistance_level'] - close) / close
            
            # Market structure
            result_df['higher_high'] = (high > np.roll(high, 1)) & (np.roll(high, 1) > np.roll(high, 2))
            result_df['lower_low'] = (low < np.roll(low, 1)) & (np.roll(low, 1) < np.roll(low, 2))
            
            # Volatility clustering
            returns = np.log(close / np.roll(close, 1))
            result_df['returns'] = returns
            result_df['volatility_5'] = pd.Series(returns).rolling(window=5).std()
            result_df['volatility_20'] = pd.Series(returns).rolling(window=20).std()
            
            # Time-based features
            if 'timestamp' in df.columns:
                df_time = pd.to_datetime(df['timestamp'])
                result_df['hour'] = df_time.dt.hour
                result_df['day_of_week'] = df_time.dt.dayofweek
                result_df['month'] = df_time.dt.month
                result_df['is_weekend'] = (df_time.dt.dayofweek >= 5).astype(int)
                
            # –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            result_df = result_df.replace([np.inf, -np.inf], np.nan)
            
            # –£–ª—É—á—à–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–ø—É—Å–∫–æ–≤
            result_df = result_df.interpolate(method='linear', limit_direction='both')
            result_df = result_df.fillna(method='bfill').fillna(method='ffill')
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞: –∑–∞–ø–æ–ª–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è NaN –Ω—É–ª—è–º–∏
            result_df = result_df.fillna(0)
            
            # –î–æ–±–∞–≤–ª—è–µ–º sentiment –∞–Ω–∞–ª–∏–∑
            try:
                sentiment_service = SentimentAnalysisService()
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–∏–º–≤–æ–ª –∏–∑ –∏–º–µ–Ω–∏ —Å—Ç–æ–ª–±—Ü–∞, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å –≤ –¥–∞–Ω–Ω—ã—Ö
                if hasattr(df, 'columns') and len(df.columns) > 0:
                    # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Å–∏–º–≤–æ–ª –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä
                    symbol_for_sentiment = getattr(df, 'symbol', symbol) if hasattr(df, 'symbol') else symbol
                else:
                    symbol_for_sentiment = symbol
                
                symbol_clean = symbol_for_sentiment.split('-')[0] if '-' in symbol_for_sentiment else symbol_for_sentiment
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ sentiment –ø—Ä–∏–∑–Ω–∞–∫–∏
                sentiment_features = sentiment_service.get_enhanced_sentiment_features(symbol_clean)
                
                # –î–æ–±–∞–≤–ª—è–µ–º sentiment –ø—Ä–∏–∑–Ω–∞–∫–∏ –∫–æ –≤—Å–µ–º —Å—Ç—Ä–æ–∫–∞–º
                for feature_name, feature_value in sentiment_features.items():
                    result_df[feature_name] = feature_value
                
                current_app.logger.info(f"üì∞ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(sentiment_features)} sentiment –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
            except Exception as e:
                current_app.logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –¥–æ–±–∞–≤–∏—Ç—å sentiment –ø—Ä–∏–∑–Ω–∞–∫–∏: {e}")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–∞–∫—Ä–æ—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            try:
                macro_service = MacroIndicatorsService()
                symbol_clean = symbol_for_sentiment.split('-')[0] if '-' in symbol_for_sentiment else symbol_for_sentiment
                macro_features = macro_service.get_enhanced_macro_features(symbol_clean)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –º–∞–∫—Ä–æ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∫–æ –≤—Å–µ–º —Å—Ç—Ä–æ–∫–∞–º
                for feature_name, feature_value in macro_features.items():
                    result_df[feature_name] = feature_value
                
                current_app.logger.info(f"üìä –î–æ–±–∞–≤–ª–µ–Ω–æ {len(macro_features)} –º–∞–∫—Ä–æ—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
            except Exception as e:
                current_app.logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –¥–æ–±–∞–≤–∏—Ç—å –º–∞–∫—Ä–æ—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {e}")
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN –∏ –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            result_df = result_df.replace([np.inf, -np.inf], 0)
            
            return result_df
            
        except Exception as e:
            current_app.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")
            raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã: {e}")
    
    @staticmethod
    def _calculate_vortex_indicator(high: np.ndarray, low: np.ndarray, close: np.ndarray, period: int = 14) -> Tuple[np.ndarray, np.ndarray]:
        """
        –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç Vortex Indicator (VI)
        
        Args:
            high: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–µ —Ü–µ–Ω—ã
            low: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ü–µ–Ω—ã  
            close: –¶–µ–Ω—ã –∑–∞–∫—Ä—ã—Ç–∏—è
            period: –ü–µ—Ä–∏–æ–¥ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞
            
        Returns:
            Tuple: (VI+, VI-)
        """
        try:
            # True Range
            tr = np.maximum(high - low, 
                           np.maximum(abs(high - np.roll(close, 1)), 
                                    abs(low - np.roll(close, 1))))
            
            # Vortex Movement
            vm_plus = abs(high - np.roll(low, 1))
            vm_minus = abs(low - np.roll(high, 1))
            
            # –°—É–º–º—ã –∑–∞ –ø–µ—Ä–∏–æ–¥
            tr_sum = pd.Series(tr).rolling(window=period).sum()
            vm_plus_sum = pd.Series(vm_plus).rolling(window=period).sum()
            vm_minus_sum = pd.Series(vm_minus).rolling(window=period).sum()
            
            # Vortex Indicator
            vi_plus = vm_plus_sum / tr_sum
            vi_minus = vm_minus_sum / tr_sum
            
            return vi_plus.values, vi_minus.values
            
        except Exception:
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –º–∞—Å—Å–∏–≤—ã –Ω—É–ª–µ–π –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
            return np.zeros(len(high)), np.zeros(len(high))


class EnsembleMLModel:
    """
    Ensemble –º–æ–¥–µ–ª—å, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è –Ω–µ—Å–∫–æ–ª—å–∫–æ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    """
    
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.feature_importance = {}
        self.is_trained = False
        self.training_metrics = {}
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≤–µ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self.weights = {'rf': 0.4, 'gb': 0.35, 'xgb': 0.25}
        self.performance_history = {'rf': [], 'gb': [], 'xgb': []}
        self.adaptive_weights_enabled = True
        self.min_history_length = 5  # –ú–∏–Ω–∏–º—É–º –∏–∑–º–µ—Ä–µ–Ω–∏–π –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –≤–µ—Å–æ–≤
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        self.hyperopt_service = None
        self.use_optimized_params = True
        
    def _prepare_models(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π ensemble —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ—Ä–≤–∏—Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        if self.hyperopt_service is None:
            self.hyperopt_service = HyperparameterOptimizationService()
        
        # Random Forest - —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π –ø—Ä–æ—Ç–∏–≤ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
        rf_params = {
            'n_estimators': 200,      # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
            'max_depth': 12,          # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –ª—É—á—à–µ–π –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏–∏
            'min_samples_split': 10,  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
            'min_samples_leaf': 5,    # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            'max_features': 0.6,      # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            'bootstrap': True,
            'oob_score': True,        # –î–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            'random_state': 42,
            'n_jobs': -1
        }
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã
        if self.use_optimized_params and 'rf' in self.hyperopt_service.best_params:
            optimized_params = self.hyperopt_service.best_params['rf'].copy()
            optimized_params.update({'random_state': 42, 'n_jobs': -1, 'oob_score': True})
            self.models['rf'] = RandomForestRegressor(**optimized_params)
            current_app.logger.info("üîß –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è RF")
        else:
            self.models['rf'] = RandomForestRegressor(**rf_params)
        
        # Gradient Boosting - —Å —Å–∏–ª—å–Ω–æ–π —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π –ø—Ä–æ—Ç–∏–≤ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
        gb_params = {
            'n_estimators': 150,      # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
            'learning_rate': 0.05,    # –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–º–µ–Ω—å—à–µ–Ω–æ
            'max_depth': 4,           # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –ª—É—á—à–µ–π –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏–∏
            'min_samples_split': 15,  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
            'min_samples_leaf': 8,    # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            'subsample': 0.7,         # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å
            'max_features': 0.5,      # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            'validation_fraction': 0.2, # –£–≤–µ–ª–∏—á–µ–Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏—è
            'n_iter_no_change': 10,   # –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞
            'random_state': 42
        }
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã
        if self.use_optimized_params and 'gb' in self.hyperopt_service.best_params:
            optimized_params = self.hyperopt_service.best_params['gb'].copy()
            optimized_params.update({'random_state': 42})
            self.models['gb'] = GradientBoostingRegressor(**optimized_params)
            current_app.logger.info("üîß –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è GB")
        else:
            self.models['gb'] = GradientBoostingRegressor(**gb_params)
        
        # XGBoost - —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π –ø—Ä–æ—Ç–∏–≤ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
        if XGBOOST_AVAILABLE:
            xgb_params = {
                'n_estimators': 150,      # –£–º–µ–Ω—å—à–µ–Ω–æ
                'learning_rate': 0.03,    # –û—á–µ–Ω—å –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å
                'max_depth': 4,           # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≥–ª—É–±–∏–Ω—É
                'min_child_weight': 6,    # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
                'subsample': 0.7,         # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å
                'colsample_bytree': 0.6,  # –ú–µ–Ω—å—à–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                'colsample_bylevel': 0.8, # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
                'reg_alpha': 0.5,         # –£–≤–µ–ª–∏—á–µ–Ω–∞ L1 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
                'reg_lambda': 2.0,        # –£–≤–µ–ª–∏—á–µ–Ω–∞ L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
                'gamma': 0.2,             # –£–≤–µ–ª–∏—á–µ–Ω –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π gain
                'objective': 'reg:squarederror',
                'eval_metric': 'rmse',
                'random_state': 42,
                'n_jobs': -1,
                'verbosity': 0
            }
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã
            if self.use_optimized_params and 'xgb' in self.hyperopt_service.best_params:
                optimized_params = self.hyperopt_service.best_params['xgb'].copy()
                optimized_params.update({
                    'objective': 'reg:squarederror',
                    'eval_metric': 'rmse',
                    'random_state': 42,
                    'n_jobs': -1,
                    'verbosity': 0
                })
                self.models['xgb'] = xgb.XGBRegressor(**optimized_params)
                current_app.logger.info("üîß –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è XGB")
            else:
                self.models['xgb'] = xgb.XGBRegressor(**xgb_params)
            
        # –°–∫–∞–ª–µ—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
        for model_name in self.models.keys():
            self.scalers[model_name] = RobustScaler()
    
    def train(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, float]:
        """
        –û–±—É—á–µ–Ω–∏–µ ensemble –º–æ–¥–µ–ª–∏
        
        Args:
            X: –ü—Ä–∏–∑–Ω–∞–∫–∏
            y: –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
            
        Returns:
            –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏
        """
        if len(X) < 100:
            raise ValueError("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–º–∏–Ω–∏–º—É–º 100 –æ–±—Ä–∞–∑—Ü–æ–≤)")
        
        current_app.logger.info(f"ü§ñ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ Ensemble –º–æ–¥–µ–ª–∏ —Å {len(X)} –æ–±—Ä–∞–∑—Ü–∞–º–∏ –∏ {len(X.columns)} –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏")
        start_time = time.time()
            
        self._prepare_models()
        current_app.logger.info(f"üìä –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(self.models)} –º–æ–¥–µ–ª–µ–π: {list(self.models.keys())}")
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—é –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        split_idx = int(len(X) * 0.8)
        X_train, X_val = X.iloc[:split_idx], X.iloc[split_idx:]
        y_train, y_val = y.iloc[:split_idx], y.iloc[split_idx:]
        
        metrics = {}
        predictions = {}
        
        for model_name, model in self.models.items():
            try:
                current_app.logger.info(f"üîÑ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ {model_name.upper()}...")
                model_start = time.time()
                
                # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                X_train_scaled = self.scalers[model_name].fit_transform(X_train)
                X_val_scaled = self.scalers[model_name].transform(X_val)
                current_app.logger.info(f"   üìè –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –¥–ª—è {model_name}")
                
                # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
                model.fit(X_train_scaled, y_train)
                training_time = time.time() - model_start
                current_app.logger.info(f"   ‚è±Ô∏è  –û–±—É—á–µ–Ω–∏–µ {model_name} –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {training_time:.2f}—Å")
                
                # Cross-validation –¥–ª—è –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–æ–π –æ—Ü–µ–Ω–∫–∏
                tscv = TimeSeriesSplit(n_splits=5)
                X_full_scaled = self.scalers[model_name].transform(X)
                cv_scores = cross_val_score(model, X_full_scaled, y, cv=tscv, scoring='neg_mean_absolute_error', n_jobs=-1)
                cv_mae = -cv_scores.mean()
                cv_std = cv_scores.std()
                current_app.logger.info(f"   üìä Cross-validation MAE: {cv_mae:.2f} ¬± {cv_std:.2f}")
                
                # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
                y_pred = model.predict(X_val_scaled)
                predictions[model_name] = y_pred
                
                # –ú–µ—Ç—Ä–∏–∫–∏
                mae = mean_absolute_error(y_val, y_pred)
                mse = mean_squared_error(y_val, y_pred)
                r2 = r2_score(y_val, y_pred)
                accuracy = max(0, min(100, (1 - mae / np.mean(y_val)) * 100))
                
                metrics[model_name] = {
                    'mae': mae,
                    'mse': mse,
                    'rmse': np.sqrt(mse),
                    'r2': r2,
                    'accuracy': accuracy,
                    'training_time': training_time,
                    'cv_mae': cv_mae,
                    'cv_std': cv_std
                }
                
                current_app.logger.info(f"   üìà {model_name.upper()} - –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.2f}%, R¬≤: {r2:.3f}, MAE: {mae:.2f}")
                
                # Feature importance
                if hasattr(model, 'feature_importances_'):
                    self.feature_importance[model_name] = dict(zip(
                        X.columns, model.feature_importances_
                    ))
                    top_features = sorted(self.feature_importance[model_name].items(), key=lambda x: x[1], reverse=True)[:5]
                    current_app.logger.info(f"   üîç –¢–æ–ø-5 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {model_name}: {[f[0] for f in top_features]}")
                    
            except Exception as e:
                current_app.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ {model_name}: {e}")
                continue
        
        # Ensemble –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ (–≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ)
        if predictions:
            weights = {name: metrics[name]['r2'] for name in predictions.keys() if metrics[name]['r2'] > 0}
            if weights:
                total_weight = sum(weights.values())
                ensemble_pred = sum(
                    pred * (weights[name] / total_weight) 
                    for name, pred in predictions.items() 
                    if name in weights
                )
                
                # –ú–µ—Ç—Ä–∏–∫–∏ ensemble
                ensemble_mae = mean_absolute_error(y_val, ensemble_pred)
                ensemble_r2 = r2_score(y_val, ensemble_pred)
                ensemble_accuracy = max(0, min(100, (1 - ensemble_mae / np.mean(y_val)) * 100))
                
                metrics['ensemble'] = {
                    'mae': ensemble_mae,
                    'r2': ensemble_r2,
                    'accuracy': ensemble_accuracy
                }
                
                current_app.logger.info(f"üéØ ENSEMBLE –†–ï–ó–£–õ–¨–¢–ê–¢ - –¢–æ—á–Ω–æ—Å—Ç—å: {ensemble_accuracy:.2f}%, R¬≤: {ensemble_r2:.3f}")
        
        total_time = time.time() - start_time
        current_app.logger.info(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ Ensemble –º–æ–¥–µ–ª–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {total_time:.2f}—Å")
        
        self.is_trained = True
        self.training_metrics = metrics  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        return metrics
    
    def predict(self, X: pd.DataFrame) -> Tuple[float, float, Dict[str, Any]]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ü–µ–Ω—ã —Å –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–º –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–º
        
        Args:
            X: –ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            
        Returns:
            Tuple: (–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è_—Ü–µ–Ω–∞, —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ_–æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ, –¥–µ—Ç–∞–ª–∏)
        """
        if not self.is_trained:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")
            
        individual_predictions = {}
        
        for model_name, model in self.models.items():
            try:
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º
                X_clean = X.fillna(0).replace([np.inf, -np.inf], 0)
                X_scaled = self.scalers[model_name].transform(X_clean)
                pred = model.predict(X_scaled)[0]
                individual_predictions[model_name] = pred
            except Exception as e:
                current_app.logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –æ—Ç {model_name}: {e}")
                continue

        if not individual_predictions:
            raise ValueError("–ù–∏ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –Ω–µ —Å–º–æ–≥–ª–∞ —Å–¥–µ–ª–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ.")

        # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ R2
        weights = {
            name: self.training_metrics[name]['r2'] 
            for name in individual_predictions.keys() 
            if name in self.training_metrics and self.training_metrics[name]['r2'] > 0
        }
        
        if not weights:
            # –ï—Å–ª–∏ –Ω–µ—Ç –≤–µ—Å–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤—Å–µ R2 < 0), –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–µ —Å—Ä–µ–¥–Ω–µ–µ
            final_prediction = np.mean(list(individual_predictions.values()))
        else:
            total_weight = sum(weights.values())
            final_prediction = sum(
                pred * (weights[name] / total_weight) 
                for name, pred in individual_predictions.items() 
                if name in weights
            )

        # –†–∞—Å—á–µ—Ç –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π)
        prediction_std = np.std(list(individual_predictions.values()))

        details = {
            'ensemble_prediction': final_prediction,
            'prediction_std_dev': prediction_std,
            'individual_predictions': individual_predictions,
            'model_weights': weights
        }

        return final_prediction, prediction_std, details
    
    def optimize_hyperparameters(
        self,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        use_reduced_grid: bool = True,
        cv_folds: int = 3
    ) -> Dict[str, Any]:
        """
        –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –≤ –∞–Ω—Å–∞–º–±–ª–µ
        """
        current_app.logger.info("üîß –ù–∞—á–∏–Ω–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∞–Ω—Å–∞–º–±–ª—è")
        
        if self.hyperopt_service is None:
            self.hyperopt_service = HyperparameterOptimizationService()
        
        optimization_results = self.hyperopt_service.optimize_ensemble_hyperparameters(
            X_train=X_train,
            y_train=y_train,
            use_reduced_grid=use_reduced_grid,
            cv_folds=cv_folds
        )
        
        if optimization_results.get('optimized_models'):
            current_app.logger.info("üîÑ –û–±–Ω–æ–≤–ª—è–µ–º –º–æ–¥–µ–ª–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏")
            self.models.update(optimization_results['optimized_models'])
            
            for model_name in self.models.keys():
                self.scalers[model_name] = RobustScaler()
        
        summary = optimization_results.get('optimization_summary', {})
        for model_name, results in summary.items():
            metrics = results.get('metrics', {})
            if 'error' not in metrics:
                current_app.logger.info(
                    f"‚úÖ {model_name.upper()}: CV MAE {metrics.get('best_cv_mae', 'N/A'):.2f}, "
                    f"R¬≤ {metrics.get('train_r2', 'N/A'):.3f}"
                )
            else:
                current_app.logger.warning(f"‚ùå {model_name.upper()}: {metrics['error']}")
        
        return optimization_results


# –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç
SUPPORTED_CRYPTO_PAIRS = [
    # –¢–æ–ø –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—ã
    'BTC-USD', 'ETH-USD', 'BNB-USD',
    # DeFi —Ç–æ–∫–µ–Ω—ã
    'UNI-USD', 'AAVE-USD', 'COMP-USD', 'MKR-USD', 'SUSHI-USD',
    # Layer 1 –±–ª–æ–∫—á–µ–π–Ω—ã
    'ADA-USD', 'SOL-USD', 'DOT-USD', 'AVAX-USD', 'ATOM-USD',
    # –ê–ª—å—Ç–∫–æ–∏–Ω—ã
    'XRP-USD', 'LTC-USD', 'BCH-USD', 'ETC-USD', 'ZEC-USD',
    # –ù–æ–≤—ã–µ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–µ
    'MATIC-USD', 'LINK-USD', 'VET-USD', 'ALGO-USD', 'FTM-USD'
]

# –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
CRYPTO_CATEGORIES = {
    'major': ['BTC-USD', 'ETH-USD', 'BNB-USD'],
    'defi': ['UNI-USD', 'AAVE-USD', 'COMP-USD', 'MKR-USD', 'SUSHI-USD'],
    'layer1': ['ADA-USD', 'SOL-USD', 'DOT-USD', 'AVAX-USD', 'ATOM-USD'],
    'altcoins': ['XRP-USD', 'LTC-USD', 'BCH-USD', 'ETC-USD', 'ZEC-USD'],
    'emerging': ['MATIC-USD', 'LINK-USD', 'VET-USD', 'ALGO-USD', 'FTM-USD']
}


class AnalysisService:
    """
    –ì–ª–∞–≤–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç
    —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ensemble –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    """
    
    def __init__(self):
        self.crypto_service = CryptoService()
        self.models_path = 'models/advanced_ml'
        self.feature_engineer = AdvancedFeatureEngineering()
        self.model_cache_ttl = timedelta(hours=24)  # –í—Ä–µ–º—è –∂–∏–∑–Ω–∏ –∫—ç—à–∞ –º–æ–¥–µ–ª–∏
        self.supported_timeframes = ['1h', '4h', '1d', '1w']  # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –º–æ–¥–µ–ª–µ–π –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        os.makedirs(self.models_path, exist_ok=True)

    def _get_model_path(self, symbol: str, timeframe: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –º–æ–¥–µ–ª–∏."""
        filename = f"ensemble_model_{symbol.replace('-', '_')}_{timeframe}.joblib"
        return os.path.join(self.models_path, filename)

    def _save_model(self, model: EnsembleMLModel, symbol: str, timeframe: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –≤ —Ñ–∞–π–ª."""
        model_path = self._get_model_path(symbol, timeframe)
        try:
            joblib.dump(model, model_path)
            current_app.logger.info(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {model_path}")
        except Exception as e:
            current_app.logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å –≤ {model_path}: {e}")

    def _load_model(self, symbol: str, timeframe: str) -> Optional[EnsembleMLModel]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏–∑ —Ñ–∞–π–ª–∞, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —É—Å—Ç–∞—Ä–µ–ª–∞."""
        model_path = self._get_model_path(symbol, timeframe)
        if not os.path.exists(model_path):
            return None

        try:
            model_age = datetime.now() - datetime.fromtimestamp(os.path.getmtime(model_path))
            if model_age > self.model_cache_ttl:
                current_app.logger.info(f"–ú–æ–¥–µ–ª—å {model_path} —É—Å—Ç–∞—Ä–µ–ª–∞ ({model_age}). –¢—Ä–µ–±—É–µ—Ç—Å—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ.")
                return None

            start_time = time.time()
            model = joblib.load(model_path)
            load_time = time.time() - start_time
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ —Å —Ç–µ–∫—É—â–∏–º–∏ –¥–æ—Å—Ç—É–ø–Ω—ã–º–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞–º–∏
            if hasattr(model, 'models') and 'xgb' in model.models and not XGBOOST_AVAILABLE:
                current_app.logger.warning("–ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ–¥–µ—Ä–∂–∏—Ç XGBoost, –Ω–æ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å.")
                return None
            
            current_app.logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {model_path} –∑–∞ {load_time:.2f}—Å")
            return model
        except Exception as e:
            current_app.logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –∏–∑ {model_path}: {e}")
            return None

    def _get_historical_data(self, symbol: str, timeframe: str, extended: bool = True) -> pd.DataFrame:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –∏ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        
        Args:
            symbol: –¢–æ—Ä–≥–æ–≤–∞—è –ø–∞—Ä–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'BTC-USD')
            timeframe: –í—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω—Ç–µ—Ä–≤–∞–ª ('1h', '4h', '1d')
            extended: –ü–æ–ª—É—á–∏—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è ML
            
        Returns:
            DataFrame —Å OHLCV –¥–∞–Ω–Ω—ã–º–∏ –∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–º –∏–Ω–¥–µ–∫—Å–æ–º
        """
        # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–µ –ø–µ—Ä–∏–æ–¥—ã –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ ML –∞–Ω–∞–ª–∏–∑–∞
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è BNB-USD - —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –æ–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö
        if 'BNB' in symbol.upper():
            if extended:
                days_map = {'1d': 730, '4h': 180, '1h': 60}  # 2 –≥–æ–¥–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è BNB
            else:
                days_map = {'1d': 365, '4h': 60, '1h': 30}
        else:
            if extended:
                days_map = {'1d': 365, '4h': 90, '1h': 30}  # –ì–æ–¥ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–Ω–µ–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            else:
                days_map = {'1d': 180, '4h': 30, '1h': 14}
            
        days = days_map.get(timeframe, 365)
        
        try:
            df = self.crypto_service.get_coin_historical_data_df(symbol, days)
            if df.empty or len(df) < 100:  # –ú–∏–Ω–∏–º—É–º 100 —Ç–æ—á–µ–∫ –¥–ª—è ML
                raise ValueError(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol} (–ø–æ–ª—É—á–µ–Ω–æ {len(df)}, –Ω—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 100)")
                
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
            required_cols = ['open', 'high', 'low', 'close', 'volume']
            if not all(col in df.columns for col in required_cols):
                missing = [col for col in required_cols if col not in df.columns]
                raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {missing}")
                
            return df
            
        except Exception as e:
            current_app.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}: {e}")
            raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ: {e}")

    def advanced_ml_analysis(self, symbol: str, timeframe: str) -> Dict[str, Any]:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π ML –∞–Ω–∞–ª–∏–∑ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ensemble –º–æ–¥–µ–ª–∏
        –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ 98%+
        
        Args:
            symbol: –°–∏–º–≤–æ–ª –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'BTC-USD')
            timeframe: –í—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω—Ç–µ—Ä–≤–∞–ª ('1h', '4h', '1d')
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏
        """
        try:
            current_app.logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ ML –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è {symbol} ({timeframe})")
            start_total_time = time.time()

            # –®–∞–≥ 1: –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å
            ensemble_model = self._load_model(symbol, timeframe)
            training_metrics = None

            try:
                # –®–∞–≥ 2: –ü–æ–ª—É—á–µ–Ω–∏–µ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
                df = self._get_historical_data(symbol, timeframe)
                df_features = self.feature_engineer.create_advanced_features(df, symbol)
                
                # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (X) –∏ —Ü–µ–ª–∏ (y)
                features = [col for col in df_features.columns if col not in ['open', 'high', 'low', 'close', 'volume', 'timestamp', 'target']]
                df_features['target'] = df_features['close'].shift(-1)
                df_features = df_features.dropna(subset=['target'])
                
                X = df_features[features]
                y = df_features['target']

                # –®–∞–≥ 3: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏, –µ—Å–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ –∫—ç—à–∞
                if not ensemble_model:
                    current_app.logger.info("–ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –∏–ª–∏ —É—Å—Ç–∞—Ä–µ–ª–∞. –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏.")
                    ensemble_model = EnsembleMLModel()
                    training_metrics = ensemble_model.train(X.iloc[:-1], y.iloc[:-1])
                    self._save_model(ensemble_model, symbol, timeframe)
                else:
                    training_metrics = ensemble_model.training_metrics
                    current_app.logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å.")

                # –®–∞–≥ 4: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                last_X = X.tail(1)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –º–æ–¥–µ–ª—å—é
                try:
                    predicted_price, std_dev, prediction_details = ensemble_model.predict(last_X)
                except Exception as model_error:
                    if "feature names" in str(model_error).lower() or "unseen at fit time" in str(model_error).lower():
                        current_app.logger.warning(f"üîÑ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {symbol}. –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
                        
                        # –ü–µ—Ä–µ–æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å —Å –Ω–æ–≤—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
                        ensemble_model = EnsembleMLModel()
                        training_metrics = ensemble_model.train(X, y)
                        
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
                        self._save_model(ensemble_model, symbol, timeframe)
                        current_app.logger.info(f"üíæ –ú–æ–¥–µ–ª—å {symbol} –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ —Å –Ω–æ–≤—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏")
                        
                        # –ü–æ–≤—Ç–æ—Ä—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –Ω–æ–≤–æ–π –º–æ–¥–µ–ª—å—é
                        predicted_price, std_dev, prediction_details = ensemble_model.predict(last_X)
                    else:
                        raise model_error
                
                # –®–∞–≥ 5: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–Ω—Å–∞–π—Ç–æ–≤ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
                last_row = df_features.tail(1).iloc[0]
                insights, recommendation = self._generate_advanced_insights(last_row, prediction_details, training_metrics)

                total_time = time.time() - start_total_time
                current_app.logger.info(f"‚úÖ –ê–Ω–∞–ª–∏–∑ –¥–ª—è {symbol} –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {total_time:.2f}—Å")

                return {
                    'status': 'success',
                    'symbol': symbol,
                    'timeframe': timeframe,
                    'current_price': last_row['close'],
                    'predicted_price': predicted_price,
                    'prediction_std_dev': std_dev,
                    'recommendation': recommendation,
                    'confidence': prediction_details.get('ensemble_confidence', 0),
                    'insights': insights,
                    'training_metrics': training_metrics,
                    'prediction_details': prediction_details,
                    'total_analysis_time': total_time
                }

            except ValueError as ve:
                current_app.logger.error(f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}: {ve}")
                return {'status': 'error', 'message': str(ve)}
            except Exception as e:
                current_app.logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ advanced_ml_analysis –¥–ª—è {symbol}: {e}", exc_info=True)
                return {'status': 'error', 'message': f"–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ {symbol}."}
        except Exception as e:
            current_app.logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ advanced_ml_analysis –¥–ª—è {symbol}: {e}", exc_info=True)
            return {'status': 'error', 'message': f"–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ {symbol}."}

    def _generate_advanced_insights(self, last_row: pd.Series, prediction_details: Dict, training_metrics: Dict):
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –∏–Ω—Å–∞–π—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ ML –∞–Ω–∞–ª–∏–∑–∞ –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
        
        Args:
            last_row: –ü–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–æ–∫–∞ —Å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏
            prediction_details: –î–µ—Ç–∞–ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç ensemble –º–æ–¥–µ–ª–∏
            training_metrics: –ú–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
            
        Returns:
            Tuple: (–æ–±—ä—è—Å–Ω–µ–Ω–∏—è, —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è)
        """
        explanation = {}
        bullish_score = 0
        bearish_score = 0
        
        try:
            # –ê–Ω–∞–ª–∏–∑ RSI (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
            if 'rsi_14' in last_row:
                rsi = last_row['rsi_14']
                if rsi > 70:
                    explanation['RSI'] = f"üî¥ –ü–µ—Ä–µ–∫—É–ø–ª–µ–Ω–Ω–æ—Å—Ç—å ({rsi:.1f}). –í—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏."
                    bearish_score += 2.5
                elif rsi < 30:
                    explanation['RSI'] = f"üü¢ –ü–µ—Ä–µ–ø—Ä–æ–¥–∞–Ω–Ω–æ—Å—Ç—å ({rsi:.1f}). –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª —Ä–æ—Å—Ç–∞."
                    bullish_score += 2.5
                else:
                    explanation['RSI'] = f"üü° –ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è –∑–æ–Ω–∞ ({rsi:.1f})."
            
            # –ê–Ω–∞–ª–∏–∑ MACD
            if 'macd' in last_row and 'macd_signal' in last_row:
                macd_cross = last_row.get('macd_cross', 0)
                if macd_cross > 0:
                    explanation['MACD'] = "üü¢ –ë—ã—á–∏–π —Å–∏–≥–Ω–∞–ª - MACD –≤—ã—à–µ —Å–∏–≥–Ω–∞–ª—å–Ω–æ–π –ª–∏–Ω–∏–∏."
                    bullish_score += 2
                else:
                    explanation['MACD'] = "üî¥ –ú–µ–¥–≤–µ–∂–∏–π —Å–∏–≥–Ω–∞–ª - MACD –Ω–∏–∂–µ —Å–∏–≥–Ω–∞–ª—å–Ω–æ–π –ª–∏–Ω–∏–∏."
                    bearish_score += 2
            
            # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
            if 'ema_20' in last_row and 'ema_50' in last_row:
                if last_row['ema_20'] > last_row['ema_50']:
                    explanation['–¢—Ä–µ–Ω–¥'] = "üü¢ –í–æ—Å—Ö–æ–¥—è—â–∏–π —Ç—Ä–µ–Ω–¥ (EMA20 > EMA50)."
                    bullish_score += 2
                else:
                    explanation['–¢—Ä–µ–Ω–¥'] = "üî¥ –ù–∏—Å—Ö–æ–¥—è—â–∏–π —Ç—Ä–µ–Ω–¥ (EMA20 < EMA50)."
                    bearish_score += 2
            
            # –ê–Ω–∞–ª–∏–∑ Bollinger Bands
            if 'bb_position_20' in last_row:
                bb_pos = last_row['bb_position_20']
                if bb_pos > 0.8:
                    explanation['Bollinger Bands'] = f"üî¥ –¶–µ–Ω–∞ —É –≤–µ—Ä—Ö–Ω–µ–π –≥—Ä–∞–Ω–∏—Ü—ã ({bb_pos:.2f}). –í–æ–∑–º–æ–∂–Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ü–∏—è."
                    bearish_score += 1.5
                elif bb_pos < 0.2:
                    explanation['Bollinger Bands'] = f"üü¢ –¶–µ–Ω–∞ —É –Ω–∏–∂–Ω–µ–π –≥—Ä–∞–Ω–∏—Ü—ã ({bb_pos:.2f}). –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª –æ—Ç—Å–∫–æ–∫–∞."
                    bullish_score += 1.5
                else:
                    explanation['Bollinger Bands'] = f"üü° –¶–µ–Ω–∞ –≤ —Å—Ä–µ–¥–Ω–µ–π –∑–æ–Ω–µ ({bb_pos:.2f})."
            
            # –ê–Ω–∞–ª–∏–∑ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
            if 'atr_percent' in last_row:
                atr_pct = last_row['atr_percent']
                if atr_pct > 5:
                    explanation['–í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å'] = f"‚ö†Ô∏è –í—ã—Å–æ–∫–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å ({atr_pct:.1f}%). –ü–æ–≤—ã—à–µ–Ω–Ω—ã–π —Ä–∏—Å–∫."
                    bearish_score += 1
                elif atr_pct < 2:
                    explanation['–í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å'] = f"üìà –ù–∏–∑–∫–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å ({atr_pct:.1f}%). –°—Ç–∞–±–∏–ª—å–Ω—ã–µ —É—Å–ª–æ–≤–∏—è."
                    bullish_score += 0.5
            
            # –ê–Ω–∞–ª–∏–∑ –æ–±—ä–µ–º–æ–≤
            if 'volume_ratio_20' in last_row:
                vol_ratio = last_row['volume_ratio_20']
                if vol_ratio > 1.5:
                    explanation['–û–±—ä–µ–º'] = f"üìä –ü–æ–≤—ã—à–µ–Ω–Ω—ã–π –æ–±—ä–µ–º —Ç–æ—Ä–≥–æ–≤ ({vol_ratio:.1f}x). –°–∏–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–µ—Å."
                    bullish_score += 1
                elif vol_ratio < 0.7:
                    explanation['–û–±—ä–µ–º'] = f"üìâ –ü–æ–Ω–∏–∂–µ–Ω–Ω—ã–π –æ–±—ä–µ–º ({vol_ratio:.1f}x). –°–ª–∞–±—ã–π –∏–Ω—Ç–µ—Ä–µ—Å."
                    bearish_score += 0.5
            
            # ML –º–æ–¥–µ–ª—å –∏–Ω—Å–∞–π—Ç—ã
            model_confidence = max(prediction_details.get('confidences', {}).values()) if prediction_details.get('confidences') else 0
            best_accuracy = max([m.get('accuracy', 0) for m in training_metrics.values() if isinstance(m, dict)])
            
            explanation['ML –ú–æ–¥–µ–ª—å'] = f"ü§ñ –¢–æ—á–Ω–æ—Å—Ç—å: {best_accuracy:.1f}%, –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {model_confidence:.2f}"
            
            if best_accuracy > 95:
                explanation['–ö–∞—á–µ—Å—Ç–≤–æ –ø—Ä–æ–≥–Ω–æ–∑–∞'] = "üéØ –û—á–µ–Ω—å –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–æ–≥–Ω–æ–∑–∞ (>95%)"
                # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤–µ—Å ML –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
                if prediction_details.get('ensemble_prediction', 0) > last_row.get('close', 0):
                    bullish_score += 3
                else:
                    bearish_score += 3
            elif best_accuracy > 90:
                explanation['–ö–∞—á–µ—Å—Ç–≤–æ –ø—Ä–æ–≥–Ω–æ–∑–∞'] = "‚úÖ –í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–æ–≥–Ω–æ–∑–∞ (>90%)"
                if prediction_details.get('ensemble_prediction', 0) > last_row.get('close', 0):
                    bullish_score += 2
                else:
                    bearish_score += 2
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            total_score = bullish_score + bearish_score
            if total_score > 0:
                bullish_ratio = bullish_score / total_score
                
                if bullish_ratio > 0.75:
                    recommendation = "üöÄ –°–ò–õ–¨–ù–ê–Ø –ü–û–ö–£–ü–ö–ê"
                elif bullish_ratio > 0.6:
                    recommendation = "üìà –ü–û–ö–£–ü–ö–ê"
                elif bullish_ratio > 0.4:
                    recommendation = "‚è∏Ô∏è –î–ï–†–ñ–ê–¢–¨"
                elif bullish_ratio > 0.25:
                    recommendation = "üìâ –ü–†–û–î–ê–ñ–ê"
                else:
                    recommendation = "üîª –°–ò–õ–¨–ù–ê–Ø –ü–†–û–î–ê–ñ–ê"
            else:
                recommendation = "‚è∏Ô∏è –î–ï–†–ñ–ê–¢–¨"
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—â—É—é –æ—Ü–µ–Ω–∫—É
            explanation['–û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞'] = f"–ë—ã—á—å–∏ —Å–∏–≥–Ω–∞–ª—ã: {bullish_score:.1f}, –ú–µ–¥–≤–µ–∂—å–∏: {bearish_score:.1f}"
            
        except Exception as e:
            current_app.logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–Ω—Å–∞–π—Ç–æ–≤: {e}")
            explanation['–û—à–∏–±–∫–∞'] = "–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã"
            recommendation = "‚è∏Ô∏è –î–ï–†–ñ–ê–¢–¨"
        
        return explanation, recommendation

    def compare_cryptocurrencies(self, symbols: List[str], timeframe: str, comparison_type: str) -> Dict[str, Any]:
        """
        –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ML –∞–Ω–∞–ª–∏–∑–∞
        
        Args:
            symbols: –°–ø–∏—Å–æ–∫ —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            timeframe: –í—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω—Ç–µ—Ä–≤–∞–ª
            comparison_type: –¢–∏–ø —Å—Ä–∞–≤–Ω–µ–Ω–∏—è ('performance', 'risk', 'ml_prediction')
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å –≥—Ä–∞—Ñ–∏–∫–∞–º–∏ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏
        """
        try:
            results = {}
            
            for symbol in symbols[:5]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 5 —Å–∏–º–≤–æ–ª–æ–≤
                try:
                    analysis = self.advanced_ml_analysis(symbol, timeframe)
                    if analysis['status'] == 'success':
                        results[symbol] = {
                            'predicted_price': analysis['predicted_price'],
                            'accuracy': analysis['training_metrics']['ensemble']['accuracy'],
                            'recommendation': analysis['recommendation'],
                            'current_price': analysis['current_price']
                        }
                except Exception as e:
                    current_app.logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ {symbol}: {e}")
                    continue
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –∏–ª–∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—É —Ä–æ—Å—Ç–∞
            if comparison_type == 'ml_prediction':
                sorted_results = sorted(
                    results.items(), 
                    key=lambda x: x[1]['accuracy'], 
                    reverse=True
                )
            else:
                sorted_results = sorted(
                    results.items(), 
                    key=lambda x: (x[1]['predicted_price'] / x[1]['current_price'] - 1) if x[1]['current_price'] > 0 else 0, 
                    reverse=True
                )
            
            return {
                'status': 'success',
                'comparison_results': dict(sorted_results),
                'summary_data': [
                    {
                        'symbol': symbol,
                        'accuracy': data['accuracy'],
                        'potential_return': (data['predicted_price'] / data['current_price'] - 1) * 100 if data['current_price'] > 0 else 0,
                        'recommendation': data['recommendation']
                    }
                    for symbol, data in sorted_results
                ],
                'plot_html': self._create_comparison_plot(dict(sorted_results))
            }
            
        except Exception as e:
            current_app.logger.error(f"–û—à–∏–±–∫–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç: {e}")
            return {'status': 'error', 'message': str(e)}
    
    def _create_comparison_plot(self, results: Dict) -> str:
        """–°–æ–∑–¥–∞–µ—Ç HTML –≥—Ä–∞—Ñ–∏–∫ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç"""
        try:
            # –ü—Ä–æ—Å—Ç–∞—è HTML —Ç–∞–±–ª–∏—Ü–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (–º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å —Å Plotly)
            html = "<div class='comparison-table'><table class='table table-striped'>"
            html += "<thead><tr><th>–°–∏–º–≤–æ–ª</th><th>–¢–æ—á–Ω–æ—Å—Ç—å ML</th><th>–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª —Ä–æ—Å—Ç–∞</th><th>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è</th></tr></thead><tbody>"
            
            for symbol, data in results.items():
                potential = (data['predicted_price'] / data['current_price'] - 1) * 100 if data['current_price'] > 0 else 0
                html += f"<tr><td>{symbol}</td><td>{data['accuracy']:.1f}%</td><td>{potential:+.1f}%</td><td>{data['recommendation']}</td></tr>"
            
            html += "</tbody></table></div>"
            return html
            
        except Exception as e:
            return f"<div class='alert alert-warning'>–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞: {e}</div>"
    
    def multi_timeframe_analysis(self, symbol: str, timeframes: List[str] = None) -> Dict[str, Any]:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –∞–Ω–∞–ª–∏–∑ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞—Ö –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∞
        
        Args:
            symbol: –°–∏–º–≤–æ–ª –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'BTC-USD')
            timeframes: –°–ø–∏—Å–æ–∫ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞ –ø–æ –∫–∞–∂–¥–æ–º—É –≤—Ä–µ–º–µ–Ω–Ω–æ–º—É –∏–Ω—Ç–µ—Ä–≤–∞–ª—É
        """
        if timeframes is None:
            timeframes = ['1h', '4h', '1d']  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º 3 –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã
        valid_timeframes = [tf for tf in timeframes if tf in self.supported_timeframes]
        
        if not valid_timeframes:
            raise ValueError(f"–ù–µ—Ç –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤. –î–æ—Å—Ç—É–ø–Ω—ã: {self.supported_timeframes}")
        
        current_app.logger.info(f"üïê –ó–∞–ø—É—Å–∫ multi-timeframe –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è {symbol} –Ω–∞ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞—Ö: {valid_timeframes}")
        
        results = {}
        predictions = []
        confidences = []
        
        for timeframe in valid_timeframes:
            try:
                current_app.logger.info(f"üìä –ê–Ω–∞–ª–∏–∑ {symbol} –Ω–∞ –∏–Ω—Ç–µ—Ä–≤–∞–ª–µ {timeframe}")
                
                # –í—ã–ø–æ–ª–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞
                result = self.advanced_ml_analysis(symbol, timeframe)
                
                if result.get('status') == 'success':
                    results[timeframe] = result
                    
                    # –°–æ–±–∏—Ä–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
                    if 'predicted_price' in result:
                        predictions.append({
                            'timeframe': timeframe,
                            'price': result['predicted_price'],
                            'confidence': result.get('confidence', 0.5),
                            'accuracy': self._get_timeframe_weight(timeframe)
                        })
                        confidences.append(result.get('confidence', 0.5))
                
                current_app.logger.info(f"‚úÖ –ê–Ω–∞–ª–∏–∑ {timeframe} –∑–∞–≤–µ—Ä—à–µ–Ω")
                
            except Exception as e:
                current_app.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ {timeframe}: {e}")
                results[timeframe] = {
                    'status': 'error',
                    'error': str(e)
                }
        
        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–∑–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤
        combined_result = self._combine_timeframe_results(results, predictions)
        
        current_app.logger.info(f"üéØ Multi-timeframe –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω. –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑: ${combined_result.get('combined_price', 'N/A')}")
        
        return {
            'status': 'success',
            'symbol': symbol,
            'timeframes_analyzed': valid_timeframes,
            'individual_results': results,
            'combined_prediction': combined_result,
            'analysis_timestamp': datetime.now().isoformat()
        }
    
    def _get_timeframe_weight(self, timeframe: str) -> float:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–µ—Å –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ –ø—Ä–∏ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        –ë–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã –∏–º–µ—é—Ç –±–æ–ª—å—à–∏–π –≤–µ—Å –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö —Ç—Ä–µ–Ω–¥–æ–≤
        """
        weights = {
            '1h': 0.2,   # –ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã–π —à—É–º
            '4h': 0.3,   # –°—Ä–µ–¥–Ω–µ—Å—Ä–æ—á–Ω—ã–µ –¥–≤–∏–∂–µ–Ω–∏—è
            '1d': 0.4,   # –û—Å–Ω–æ–≤–Ω–æ–π —Ç—Ä–µ–Ω–¥
            '1w': 0.5    # –î–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–π —Ç—Ä–µ–Ω–¥
        }
        return weights.get(timeframe, 0.25)
    
    def _combine_timeframe_results(self, results: Dict, predictions: List[Dict]) -> Dict[str, Any]:
        """
        –ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Å —Ä–∞–∑–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤
        """
        if not predictions:
            return {'error': '–ù–µ—Ç —É—Å–ø–µ—à–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –¥–ª—è –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è'}
        
        # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        total_weight = 0
        weighted_price_sum = 0
        weighted_confidence_sum = 0
        
        for pred in predictions:
            weight = pred['accuracy'] * pred['confidence']
            weighted_price_sum += pred['price'] * weight
            weighted_confidence_sum += pred['confidence'] * weight
            total_weight += weight
        
        if total_weight == 0:
            return {'error': '–ù—É–ª–µ–≤–æ–π –æ–±—â–∏–π –≤–µ—Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π'}
        
        combined_price = weighted_price_sum / total_weight
        combined_confidence = weighted_confidence_sum / total_weight
        
        # –ê–Ω–∞–ª–∏–∑ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        prices = [pred['price'] for pred in predictions]
        price_std = np.std(prices)
        price_range = max(prices) - min(prices)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏
        consistency_score = 1.0 - min(price_range / combined_price, 1.0)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        recommendation = self._generate_multi_timeframe_recommendation(
            predictions, combined_confidence, consistency_score
        )
        
        return {
            'combined_price': combined_price,
            'combined_confidence': combined_confidence,
            'consistency_score': consistency_score,
            'price_range': price_range,
            'price_std': price_std,
            'recommendation': recommendation,
            'timeframe_count': len(predictions)
        }
    
    def _generate_multi_timeframe_recommendation(self, predictions: List[Dict], 
                                               combined_confidence: float, 
                                               consistency_score: float) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ multi-timeframe –∞–Ω–∞–ª–∏–∑–∞
        """
        if combined_confidence < 0.3:
            return "–û–°–¢–û–†–û–ñ–ù–û: –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –ø—Ä–æ–≥–Ω–æ–∑–µ"
        
        if consistency_score < 0.5:
            return "–í–ù–ò–ú–ê–ù–ò–ï: –ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤—ã–µ —Å–∏–≥–Ω–∞–ª—ã –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞—Ö"
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–≤–∏–∂–µ–Ω–∏—è –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞—Ö
        short_term = [p for p in predictions if p['timeframe'] in ['1h', '4h']]
        long_term = [p for p in predictions if p['timeframe'] in ['1d', '1w']]
        
        if short_term and long_term:
            short_avg = np.mean([p['price'] for p in short_term])
            long_avg = np.mean([p['price'] for p in long_term])
            
            if abs(short_avg - long_avg) / long_avg > 0.05:  # –†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ –±–æ–ª–µ–µ 5%
                if short_avg > long_avg:
                    return "–ü–û–ö–£–ü–ö–ê: –ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã–π –∏–º–ø—É–ª—å—Å –≤–≤–µ—Ä—Ö –ø—Ä–∏ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–º —Ç—Ä–µ–Ω–¥–µ"
                else:
                    return "–ü–†–û–î–ê–ñ–ê: –ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–∞—è –∫–æ—Ä—Ä–µ–∫—Ü–∏—è –≤ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–º —Ç—Ä–µ–Ω–¥–µ"
        
        if combined_confidence > 0.7 and consistency_score > 0.8:
            return "–°–ò–õ–¨–ù–´–ô –°–ò–ì–ù–ê–õ: –í—ã—Å–æ–∫–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –Ω–∞ –≤—Å–µ—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞—Ö"
        elif combined_confidence > 0.5 and consistency_score > 0.6:
            return "–£–ú–ï–†–ï–ù–ù–´–ô –°–ò–ì–ù–ê–õ: –•–æ—Ä–æ—à–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –ø—Ä–æ–≥–Ω–æ–∑–æ–≤"
        else:
            return "–ù–ï–ô–¢–†–ê–õ–¨–ù–û: –°–º–µ—à–∞–Ω–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã, —Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑"
    
    def optimize_hyperparameters(
        self, 
        symbol: str,
        timeframe: str = '1d',
        use_reduced_grid: bool = True,
        cv_folds: int = 3
    ) -> Dict[str, Any]:
        """
        –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –º–æ–¥–µ–ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—ã
        
        Args:
            symbol: –¢–æ—Ä–≥–æ–≤–∞—è –ø–∞—Ä–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'BTC-USD')
            timeframe: –í—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω—Ç–µ—Ä–≤–∞–ª
            use_reduced_grid: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —É–º–µ–Ω—å—à–µ–Ω–Ω—É—é —Å–µ—Ç–∫—É –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            cv_folds: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ–ª–¥–æ–≤ –¥–ª—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        """
        current_app.logger.info(f"üîß –ù–∞—á–∏–Ω–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è {symbol}")
        
        # –ü–æ–ª—É—á–∞–µ–º –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
        df = self._get_historical_data(symbol, timeframe, extended=True)
        if df.empty:
            raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è {symbol}")
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        df_features = AdvancedFeatureEngineering.create_advanced_features(df, symbol)
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        X = df_features.drop(['target'], axis=1, errors='ignore')
        y = df_features['close'].shift(-1).dropna()  # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Å–ª–µ–¥—É—é—â—É—é —Ü–µ–Ω—É
        X = X.iloc[:-1]  # –£–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å—Ç—Ä–æ–∫—É –∏–∑ X
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
        model = EnsembleMLModel()
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é
        optimization_results = model.optimize_hyperparameters(
            X_train=X,
            y_train=y,
            use_reduced_grid=use_reduced_grid,
            cv_folds=cv_folds
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        optimization_path = f"models/advanced_ml/hyperopt_results_{symbol.replace('-', '_')}_{timeframe}.joblib"
        os.makedirs(os.path.dirname(optimization_path), exist_ok=True)
        model.hyperopt_service.save_optimization_results(optimization_path)
        
        current_app.logger.info(f"‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è {symbol} –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        
        return optimization_results
    
    def get_supported_crypto_pairs(self) -> List[str]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–Ω—ã—Ö –ø–∞—Ä
        """
        return SUPPORTED_CRYPTO_PAIRS.copy()
    
    def get_crypto_categories(self) -> Dict[str, List[str]]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç
        """
        return CRYPTO_CATEGORIES.copy()
    
    def analyze_crypto_category(
        self, 
        category: str, 
        timeframe: str = '1d',
        limit: int = None
    ) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤—Å—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç
        
        Args:
            category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç ('major', 'defi', 'layer1', 'altcoins', 'emerging')
            timeframe: –í—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω—Ç–µ—Ä–≤–∞–ª
            limit: –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º—ã—Ö –ø–∞—Ä
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞ –ø–æ –∫–∞–∂–¥–æ–π –ø–∞—Ä–µ
        """
        if category not in CRYPTO_CATEGORIES:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è: {category}")
        
        pairs = CRYPTO_CATEGORIES[category]
        if limit:
            pairs = pairs[:limit]
        
        current_app.logger.info(f"üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é {category}: {len(pairs)} –ø–∞—Ä")
        
        results = {}
        successful_analyses = 0
        
        for pair in pairs:
            try:
                current_app.logger.info(f"üìä –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º {pair}...")
                result = self.advanced_ml_analysis(pair, timeframe)
                results[pair] = result
                successful_analyses += 1
                
                # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∞–Ω–∞–ª–∏–∑–∞–º–∏
                import time
                time.sleep(1)
                
            except Exception as e:
                current_app.logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å {pair}: {e}")
                results[pair] = {'error': str(e)}
        
        current_app.logger.info(f"‚úÖ –ê–Ω–∞–ª–∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {category} –∑–∞–≤–µ—Ä—à–µ–Ω: {successful_analyses}/{len(pairs)} —É—Å–ø–µ—à–Ω–æ")
        
        return {
            'category': category,
            'total_pairs': len(pairs),
            'successful_analyses': successful_analyses,
            'results': results,
            'summary': self._generate_category_summary(results)
        }
    
    def _generate_category_summary(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–≤–æ–¥–∫—É –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç
        """
        successful_results = {k: v for k, v in results.items() if 'error' not in v}
        
        if not successful_results:
            return {'message': '–ù–µ—Ç —É—Å–ø–µ—à–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–≤–æ–¥–∫–∏'}
        
        # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        predictions = []
        confidences = []
        price_changes = []
        
        for pair, result in successful_results.items():
            if 'predicted_price' in result and 'current_price' in result:
                current_price = result['current_price']
                predicted_price = result['predicted_price']
                
                if current_price > 0:
                    change_percent = ((predicted_price - current_price) / current_price) * 100
                    price_changes.append(change_percent)
                    predictions.append(predicted_price)
                
                if 'confidence' in result:
                    confidences.append(result['confidence'])
        
        summary = {
            'successful_pairs': len(successful_results),
            'average_confidence': np.mean(confidences) if confidences else 0,
            'average_price_change': np.mean(price_changes) if price_changes else 0,
            'bullish_pairs': len([c for c in price_changes if c > 0]) if price_changes else 0,
            'bearish_pairs': len([c for c in price_changes if c < 0]) if price_changes else 0,
            'neutral_pairs': len([c for c in price_changes if abs(c) < 1]) if price_changes else 0
        }
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–±—â–∏–π —Ç—Ä–µ–Ω–¥ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        if summary['average_price_change'] > 2:
            summary['category_trend'] = '–ë–´–ß–ò–ô'
        elif summary['average_price_change'] < -2:
            summary['category_trend'] = '–ú–ï–î–í–ï–ñ–ò–ô'
        else:
            summary['category_trend'] = '–ù–ï–ô–¢–†–ê–õ–¨–ù–´–ô'
        
        return summary
