# app/services/backtesting_service.py
"""
–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –±—ç–∫—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ ML-–º–æ–¥–µ–ª–µ–π –¥–ª—è –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
"""
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Any
from datetime import datetime, timedelta
import warnings
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error
import joblib
import os
from flask import current_app

from app.services.analysis_service import AnalysisService, EnsembleMLModel, AdvancedFeatureEngineering
from app.services.crypto_service import CryptoService

warnings.filterwarnings('ignore')


class BacktestingService:
    """
    –°–µ—Ä–≤–∏—Å –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –±—ç–∫—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è ML-–º–æ–¥–µ–ª–µ–π
    """
    
    def __init__(self):
        self.crypto_service = CryptoService()
        self.analysis_service = AnalysisService()
        self.feature_engineer = AdvancedFeatureEngineering()
        
    def comprehensive_backtest(self, symbol: str, timeframe: str, 
                             test_periods: int = 30, 
                             retrain_frequency: int = 7) -> Dict[str, Any]:
        """
        –ü—Ä–æ–≤–æ–¥–∏—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –±—ç–∫—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        
        Args:
            symbol: –¢–æ—Ä–≥–æ–≤–∞—è –ø–∞—Ä–∞
            timeframe: –í—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω—Ç–µ—Ä–≤–∞–ª
            test_periods: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–µ—Ä–∏–æ–¥–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            retrain_frequency: –ß–∞—Å—Ç–æ—Ç–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ (–≤ –¥–Ω—è—Ö)
            
        Returns:
            –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏
        """
        current_app.logger.info(f"üî¨ –ù–∞—á–∏–Ω–∞–µ–º –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –±—ç–∫—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è {symbol} ({timeframe})")
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
        df = self._get_extended_historical_data(symbol, timeframe, periods=365)
        
        if len(df) < 100:
            raise ValueError(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±—ç–∫—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {len(df)} –∑–∞–ø–∏—Å–µ–π")
            
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        df_features = self.feature_engineer.create_advanced_features(df)
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        features = [col for col in df_features.columns 
                   if col not in ['open', 'high', 'low', 'close', 'volume', 'timestamp', 'target']]
        
        df_features['target'] = df_features['close'].shift(-1)
        df_features = df_features.dropna(subset=['target'])
        
        X = df_features[features]
        y = df_features['target']
        
        # –ü—Ä–æ–≤–æ–¥–∏–º walk-forward –∞–Ω–∞–ª–∏–∑
        results = self._walk_forward_analysis(X, y, test_periods, retrain_frequency)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        performance_metrics = self._calculate_performance_metrics(results)
        
        # –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        charts = self._create_performance_charts(results, symbol)
        
        # –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        feature_importance = self._analyze_feature_importance(X, y)
        
        current_app.logger.info(f"‚úÖ –ë—ç–∫—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –¢–æ—á–Ω–æ—Å—Ç—å: {performance_metrics['accuracy']:.2f}%")
        
        return {
            'symbol': symbol,
            'timeframe': timeframe,
            'test_periods': test_periods,
            'performance_metrics': performance_metrics,
            'predictions_vs_actual': results,
            'feature_importance': feature_importance,
            'charts': charts,
            'recommendations': self._generate_model_recommendations(performance_metrics)
        }
    
    def _get_extended_historical_data(self, symbol: str, timeframe: str, periods: int = 365) -> pd.DataFrame:
        """–ü–æ–ª—É—á–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –º–µ—Ç–æ–¥ –∏–∑ analysis_service
            return self.analysis_service._get_historical_data(symbol, timeframe, extended=True)
        except Exception as e:
            current_app.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: {e}")
            # –°–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            return self._generate_synthetic_data(periods)
    
    def _generate_synthetic_data(self, periods: int) -> pd.DataFrame:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        dates = pd.date_range(start=datetime.now() - timedelta(days=periods), 
                             end=datetime.now(), freq='D')
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ —Ü–µ–Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Å —Ç—Ä–µ–Ω–¥–æ–º –∏ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å—é
        np.random.seed(42)
        base_price = 50000
        returns = np.random.normal(0.001, 0.02, len(dates))  # –°—Ä–µ–¥–Ω—è—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å 0.1% —Å –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å—é 2%
        
        prices = [base_price]
        for ret in returns[1:]:
            prices.append(prices[-1] * (1 + ret))
        
        # –°–æ–∑–¥–∞–µ–º OHLCV –¥–∞–Ω–Ω—ã–µ
        df = pd.DataFrame({
            'timestamp': dates,
            'open': prices,
            'high': [p * (1 + abs(np.random.normal(0, 0.01))) for p in prices],
            'low': [p * (1 - abs(np.random.normal(0, 0.01))) for p in prices],
            'close': prices,
            'volume': np.random.uniform(1000000, 10000000, len(dates))
        })
        
        # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º high/low
        df['high'] = np.maximum(df['high'], np.maximum(df['open'], df['close']))
        df['low'] = np.minimum(df['low'], np.minimum(df['open'], df['close']))
        
        return df
    
    def _walk_forward_analysis(self, X: pd.DataFrame, y: pd.Series, 
                              test_periods: int, retrain_frequency: int) -> List[Dict]:
        """
        –ü—Ä–æ–≤–æ–¥–∏—Ç walk-forward –∞–Ω–∞–ª–∏–∑ –º–æ–¥–µ–ª–∏
        """
        results = []
        total_samples = len(X)
        
        # –ù–∞—á–∏–Ω–∞–µ–º —Å 70% –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        initial_train_size = int(total_samples * 0.7)
        
        current_app.logger.info(f"üìä Walk-forward –∞–Ω–∞–ª–∏–∑: {test_periods} –ø–µ—Ä–∏–æ–¥–æ–≤, –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ {retrain_frequency} –¥–Ω–µ–π")
        
        model = None
        days_since_retrain = 0
        
        for i in range(initial_train_size, min(total_samples - 1, initial_train_size + test_periods)):
            # –ü–µ—Ä–µ–æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
            if model is None or days_since_retrain >= retrain_frequency:
                current_app.logger.info(f"üîÑ –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –ø–µ—Ä–∏–æ–¥–µ {i}")
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
                train_start = max(0, i - 200)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 200 —Ç–æ—á–µ–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
                X_train = X.iloc[train_start:i]
                y_train = y.iloc[train_start:i]
                
                model = EnsembleMLModel()
                try:
                    training_metrics = model.train(X_train, y_train)
                    days_since_retrain = 0
                except Exception as e:
                    current_app.logger.error(f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
                    continue
            
            # –î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            try:
                X_test = X.iloc[i:i+1]
                actual_price = y.iloc[i]
                
                predicted_price, std_dev, details = model.predict(X_test)
                
                # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
                error = abs(predicted_price - actual_price)
                percentage_error = (error / actual_price) * 100
                
                direction_actual = 1 if i > 0 and actual_price > y.iloc[i-1] else 0
                direction_predicted = 1 if i > 0 and predicted_price > y.iloc[i-1] else 0
                direction_correct = direction_actual == direction_predicted
                
                results.append({
                    'period': i,
                    'actual_price': actual_price,
                    'predicted_price': predicted_price,
                    'error': error,
                    'percentage_error': percentage_error,
                    'direction_correct': direction_correct,
                    'std_dev': std_dev,
                    'confidence': 1 / (1 + percentage_error)  # –ü—Ä–æ—Å—Ç–∞—è –º–µ—Ä–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
                })
                
                days_since_retrain += 1
                
            except Exception as e:
                current_app.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –ø–µ—Ä–∏–æ–¥–µ {i}: {e}")
                continue
        
        return results
    
    def _calculate_performance_metrics(self, results: List[Dict]) -> Dict[str, float]:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏"""
        if not results:
            return {}
        
        df_results = pd.DataFrame(results)
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏
        mae = df_results['error'].mean()
        rmse = np.sqrt(df_results['error'].pow(2).mean())
        mape = df_results['percentage_error'].mean()
        
        # –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è
        direction_accuracy = df_results['direction_correct'].mean() * 100
        
        # R¬≤ score
        actual_prices = df_results['actual_price'].values
        predicted_prices = df_results['predicted_price'].values
        r2 = r2_score(actual_prices, predicted_prices)
        
        # –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        returns_actual = np.diff(actual_prices) / actual_prices[:-1]
        returns_predicted = np.diff(predicted_prices) / actual_prices[:-1]
        
        # Sharpe ratio (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π)
        sharpe_ratio = np.mean(returns_predicted) / np.std(returns_predicted) if np.std(returns_predicted) > 0 else 0
        
        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–æ—Å–∞–¥–∫–∞
        cumulative_returns = np.cumprod(1 + returns_predicted)
        running_max = np.maximum.accumulate(cumulative_returns)
        drawdown = (cumulative_returns - running_max) / running_max
        max_drawdown = np.min(drawdown) * 100
        
        # –û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (–∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞)
        accuracy = max(0, min(100, (1 - mape/100) * 100))
        
        return {
            'accuracy': accuracy,
            'mae': mae,
            'rmse': rmse,
            'mape': mape,
            'direction_accuracy': direction_accuracy,
            'r2_score': r2,
            'sharpe_ratio': sharpe_ratio,
            'max_drawdown': max_drawdown,
            'total_predictions': len(results),
            'confidence_avg': df_results['confidence'].mean()
        }
    
    def _analyze_feature_importance(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, float]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        try:
            from sklearn.ensemble import RandomForestRegressor
            
            # –û–±—É—á–∞–µ–º –ø—Ä–æ—Å—Ç—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            rf = RandomForestRegressor(n_estimators=100, random_state=42)
            rf.fit(X.fillna(0), y)
            
            importance_dict = dict(zip(X.columns, rf.feature_importances_))
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
            sorted_importance = dict(sorted(importance_dict.items(), 
                                          key=lambda x: x[1], reverse=True))
            
            return sorted_importance
            
        except Exception as e:
            current_app.logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")
            return {}
    
    def _create_performance_charts(self, results: List[Dict], symbol: str) -> Dict[str, str]:
        """–°–æ–∑–¥–∞–µ—Ç –≥—Ä–∞—Ñ–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏"""
        try:
            df_results = pd.DataFrame(results)
            
            # –ì—Ä–∞—Ñ–∏–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π vs —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
            plt.figure(figsize=(15, 10))
            
            # Subplot 1: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è vs –†–µ–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            plt.subplot(2, 2, 1)
            plt.plot(df_results['actual_price'], label='–†–µ–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞', alpha=0.7)
            plt.plot(df_results['predicted_price'], label='–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è —Ü–µ–Ω–∞', alpha=0.7)
            plt.title(f'–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è vs –†–µ–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è - {symbol}')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            # Subplot 2: –û—à–∏–±–∫–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
            plt.subplot(2, 2, 2)
            plt.plot(df_results['percentage_error'])
            plt.title('–ü—Ä–æ—Ü–µ–Ω—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π')
            plt.ylabel('–û—à–∏–±–∫–∞ (%)')
            plt.grid(True, alpha=0.3)
            
            # Subplot 3: –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
            plt.subplot(2, 2, 3)
            rolling_accuracy = df_results['direction_correct'].rolling(window=10).mean() * 100
            plt.plot(rolling_accuracy)
            plt.title('–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è (—Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ 10 –ø–µ—Ä–∏–æ–¥–æ–≤)')
            plt.ylabel('–¢–æ—á–Ω–æ—Å—Ç—å (%)')
            plt.grid(True, alpha=0.3)
            
            # Subplot 4: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫
            plt.subplot(2, 2, 4)
            plt.hist(df_results['percentage_error'], bins=20, alpha=0.7, edgecolor='black')
            plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã—Ö –æ—à–∏–±–æ–∫')
            plt.xlabel('–û—à–∏–±–∫–∞ (%)')
            plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
            plt.grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫
            chart_path = f'static/charts/backtest_{symbol}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.png'
            os.makedirs(os.path.dirname(chart_path), exist_ok=True)
            plt.savefig(chart_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            return {'performance_chart': chart_path}
            
        except Exception as e:
            current_app.logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤: {e}")
            return {}
    
    def _generate_model_recommendations(self, metrics: Dict[str, float]) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é –º–æ–¥–µ–ª–∏"""
        recommendations = []
        
        accuracy = metrics.get('accuracy', 0)
        direction_accuracy = metrics.get('direction_accuracy', 0)
        r2_score = metrics.get('r2_score', 0)
        sharpe_ratio = metrics.get('sharpe_ratio', 0)
        
        if accuracy < 85:
            recommendations.append("üîß –ù–∏–∑–∫–∞—è –æ–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–ª–∏ —É–ª—É—á—à–∏—Ç—å –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö.")
        
        if direction_accuracy < 60:
            recommendations.append("üìà –ù–∏–∑–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è. –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ momentum –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤.")
        
        if r2_score < 0.3:
            recommendations.append("üìä –ù–∏–∑–∫–∏–π R¬≤ score. –ú–æ–¥–µ–ª—å –ø–ª–æ—Ö–æ –æ–±—ä—è—Å–Ω—è–µ—Ç –≤–∞—Ä–∏–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã.")
        
        if sharpe_ratio < 0.5:
            recommendations.append("üí∞ –ù–∏–∑–∫–∏–π Sharpe ratio. –°—Ç—Ä–∞—Ç–µ–≥–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ—Ä–µ–Ω—Ç–∞–±–µ–ª—å–Ω–æ–π —Å —É—á–µ—Ç–æ–º —Ä–∏—Å–∫–∞.")
        
        if accuracy > 90 and direction_accuracy > 70:
            recommendations.append("‚úÖ –û—Ç–ª–∏—á–Ω–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏! –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ.")
        
        if not recommendations:
            recommendations.append("üìã –ú–æ–¥–µ–ª—å –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã. –ü—Ä–æ–¥–æ–ª–∂–∞–π—Ç–µ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.")
        
        return recommendations
    
    def cross_validation_analysis(self, symbol: str, timeframe: str, cv_folds: int = 5) -> Dict[str, Any]:
        """
        –ü—Ä–æ–≤–æ–¥–∏—Ç cross-validation –∞–Ω–∞–ª–∏–∑ –º–æ–¥–µ–ª–∏
        """
        current_app.logger.info(f"üîÑ Cross-validation –∞–Ω–∞–ª–∏–∑ –¥–ª—è {symbol} —Å {cv_folds} —Ñ–æ–ª–¥–∞–º–∏")
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        df = self._get_extended_historical_data(symbol, timeframe)
        df_features = self.feature_engineer.create_advanced_features(df)
        
        features = [col for col in df_features.columns 
                   if col not in ['open', 'high', 'low', 'close', 'volume', 'timestamp', 'target']]
        
        df_features['target'] = df_features['close'].shift(-1)
        df_features = df_features.dropna(subset=['target'])
        
        X = df_features[features].fillna(0)
        y = df_features['target']
        
        # Time Series Split –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
        tscv = TimeSeriesSplit(n_splits=cv_folds)
        
        cv_results = []
        
        for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):
            current_app.logger.info(f"üìä –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–æ–ª–¥–∞ {fold + 1}/{cv_folds}")
            
            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
            
            # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
            model = EnsembleMLModel()
            try:
                training_metrics = model.train(X_train, y_train)
                
                # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ
                fold_results = []
                for i in range(len(X_test)):
                    try:
                        pred_price, std_dev, details = model.predict(X_test.iloc[i:i+1])
                        actual_price = y_test.iloc[i]
                        
                        error = abs(pred_price - actual_price)
                        percentage_error = (error / actual_price) * 100
                        
                        fold_results.append({
                            'predicted': pred_price,
                            'actual': actual_price,
                            'error': error,
                            'percentage_error': percentage_error
                        })
                    except Exception as e:
                        continue
                
                if fold_results:
                    fold_df = pd.DataFrame(fold_results)
                    fold_metrics = {
                        'fold': fold + 1,
                        'mae': fold_df['error'].mean(),
                        'mape': fold_df['percentage_error'].mean(),
                        'r2': r2_score(fold_df['actual'], fold_df['predicted']),
                        'samples': len(fold_results)
                    }
                    cv_results.append(fold_metrics)
                    
            except Exception as e:
                current_app.logger.error(f"–û—à–∏–±–∫–∞ –≤ —Ñ–æ–ª–¥–µ {fold + 1}: {e}")
                continue
        
        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        if cv_results:
            cv_df = pd.DataFrame(cv_results)
            aggregated_metrics = {
                'mean_mae': cv_df['mae'].mean(),
                'std_mae': cv_df['mae'].std(),
                'mean_mape': cv_df['mape'].mean(),
                'std_mape': cv_df['mape'].std(),
                'mean_r2': cv_df['r2'].mean(),
                'std_r2': cv_df['r2'].std(),
                'cv_folds': cv_folds,
                'fold_results': cv_results
            }
            
            current_app.logger.info(f"‚úÖ Cross-validation –∑–∞–≤–µ—Ä—à–µ–Ω. –°—Ä–µ–¥–Ω—è—è MAPE: {aggregated_metrics['mean_mape']:.2f}%")
            return aggregated_metrics
        else:
            return {'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å cross-validation'}


def run_comprehensive_model_evaluation(symbol: str = 'BTC-USD', timeframe: str = '1d') -> Dict[str, Any]:
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—É—é –æ—Ü–µ–Ω–∫—É –º–æ–¥–µ–ª–∏ —Å –±—ç–∫—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ cross-validation
    """
    backtesting_service = BacktestingService()
    
    results = {
        'timestamp': datetime.now().isoformat(),
        'symbol': symbol,
        'timeframe': timeframe
    }
    
    try:
        # –ë—ç–∫—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
        current_app.logger.info("üöÄ –ó–∞–ø—É—Å–∫ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –±—ç–∫—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è...")
        backtest_results = backtesting_service.comprehensive_backtest(
            symbol=symbol, 
            timeframe=timeframe,
            test_periods=30,
            retrain_frequency=7
        )
        results['backtest'] = backtest_results
        
        # Cross-validation
        current_app.logger.info("üîÑ –ó–∞–ø—É—Å–∫ cross-validation –∞–Ω–∞–ª–∏–∑–∞...")
        cv_results = backtesting_service.cross_validation_analysis(
            symbol=symbol,
            timeframe=timeframe,
            cv_folds=5
        )
        results['cross_validation'] = cv_results
        
        # –û–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        results['overall_recommendations'] = _generate_overall_recommendations(
            backtest_results.get('performance_metrics', {}),
            cv_results
        )
        
        current_app.logger.info("‚úÖ –ü–æ–ª–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        
    except Exception as e:
        current_app.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –º–æ–¥–µ–ª–∏: {e}")
        results['error'] = str(e)
    
    return results


def _generate_overall_recommendations(backtest_metrics: Dict, cv_metrics: Dict) -> List[str]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤"""
    recommendations = []
    
    bt_accuracy = backtest_metrics.get('accuracy', 0)
    cv_mape = cv_metrics.get('mean_mape', 100)
    cv_std = cv_metrics.get('std_mape', 0)
    
    if bt_accuracy > 85 and cv_mape < 15:
        recommendations.append("üéØ –û–¢–õ–ò–ß–ù–ê–Ø –ú–û–î–ï–õ–¨: –í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –±—ç–∫—Ç–µ—Å—Ç–µ –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –Ω–∞ cross-validation")
    elif bt_accuracy > 75 and cv_mape < 25:
        recommendations.append("‚úÖ –•–û–†–û–®–ê–Ø –ú–û–î–ï–õ–¨: –ü—Ä–∏–µ–º–ª–µ–º–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å, –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç—å—é")
    else:
        recommendations.append("‚ö†Ô∏è –¢–†–ï–ë–£–ï–¢ –£–õ–£–ß–®–ï–ù–ò–Ø: –ù–∏–∑–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å, –Ω–µ–æ–±—Ö–æ–¥–∏–º–∞ –¥–æ—Ä–∞–±–æ—Ç–∫–∞")
    
    if cv_std > 10:
        recommendations.append("üìä –í—ã—Å–æ–∫–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏")
    
    return recommendations
